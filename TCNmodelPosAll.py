import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

""" Modern TCN æ¨è + ç»“æ„é‡å‚æ•°åŒ–
ç»Ÿä¸€çš„å¤§å°å°ºåº¦ç‰¹å¾ emb è¾“å…¥

~~æ—¶é—´å·ç§¯ä½¿ç”¨'replicate'å°†çŸ©é˜µçš„è¾¹ç¼˜å¤åˆ¶å¹¶å¡«å……åˆ°çŸ©é˜µçš„å¤–å›´ã€‚~~
ï¼ˆåŸæœ¬æ˜¯ zero å¡«å……ï¼‰
"""


class Embedding(nn.Module):
    def __init__(self, P=8, S=4, D=2048):
        super(Embedding, self).__init__()
        self.P = P
        self.S = S
        self.conv = nn.Conv1d(
            in_channels=1,
            out_channels=D,
            kernel_size=P,
            stride=S
        )

    def forward(self, x):
        # x: [B, M, L]
        B = x.shape[0]
        x = x.unsqueeze(2)  # [B, M, L] -> [B, M, 1, L]
        x = rearrange(x, 'b m r l -> (b m) r l')  # [B, M, 1, L] -> [B*M, 1, L]
        x_pad = F.pad(
            x,
            pad=(0, self.P-self.S),
            mode='replicate'
        )  # [B*M, 1, L] -> [B*M, 1, L+P-S]

        x_emb = self.conv(x_pad)  # [B*M, 1, L+P-S] -> [B*M, D, N]
        # [B*M, D, N] -> [B, M, D, N]
        x_emb = rearrange(x_emb, '(b m) d n -> b m d n', b=B)

        return x_emb  # x_emb: [B, M, D, N]


class EmbeddingPos(nn.Module):
    def __init__(self, pos_D, P=1, S=1, D=256):
        super(EmbeddingPos, self).__init__()
        self.P = P
        self.S = S
        self.conv = nn.Conv1d(
            in_channels=pos_D,
            out_channels=D,
            kernel_size=P,
            stride=S
        )

    def forward(self, x):
        # x: [B, M, pos_D, L]
        B = x.shape[0]
        # [B, M, pos_D, L] -> [B*M, pos_D, L]
        x = rearrange(x, 'b m r l -> (b m) r l')
        x_pad = F.pad(
            x,
            pad=(0, self.P-self.S),
            mode='replicate'
        )  # [B*M, pos_D, L] -> [B*M, pos_D, L+P-S]

        x_emb = self.conv(x_pad)  # [B*M, pos_D, L+P-S] -> [B*M, D, N]
        # [B*M, D, N] -> [B, M, D, N]
        x_emb = rearrange(x_emb, '(b m) d n -> b m d n', b=B)

        return x_emb  # x_emb: [B, M, D, N]


class ConvFFN(nn.Module):
    def __init__(self, M, D, r, one=True):  # one is True: ConvFFN1, one is False: ConvFFN2
        super(ConvFFN, self).__init__()
        groups_num = M if one else D
        self.pw_con1 = nn.Conv1d(
            in_channels=M*D,
            out_channels=r*M*D,
            kernel_size=1,
            groups=groups_num
        )
        self.pw_con2 = nn.Conv1d(
            in_channels=r*M*D,
            out_channels=M*D,
            kernel_size=1,
            groups=groups_num
        )

    def forward(self, x):
        # x: [B, M*D, N]
        x = self.pw_con2(F.gelu(self.pw_con1(x)))
        return x  # x: [B, M*D, N]

# backbone çš„ä¸»ç»“æ„


class ModernTCNBlock(nn.Module):
    def __init__(self, M, D, kernel_size, r):
        super(ModernTCNBlock, self).__init__()
        # æ·±åº¦åˆ†ç¦»å·ç§¯è´Ÿè´£æ•è·æ—¶åŸŸå…³ç³»
        self.dw_conv = nn.Conv1d(
            in_channels=M*D,
            out_channels=M*D,
            kernel_size=kernel_size,
            groups=M*D,
            padding='same',
        )
        # ç»“æ„é‡å‚æ•°åŒ– small size = 5
        self.dw_conv_small = nn.Conv1d(
            in_channels=M*D,
            out_channels=M*D,
            kernel_size=5,
            groups=M*D,
            padding='same'
        )
        self.bn = nn.BatchNorm1d(M*D)
        self.bn_samll = nn.BatchNorm1d(M*D)
        self.conv_ffn1 = ConvFFN(M, D, r, one=True)
        self.conv_ffn2 = ConvFFN(M, D, r, one=False)

    def forward(self, x_emb):
        # x_emb: [B, M, D, N] ç‰¹å¾åµŒå…¥ï¼ˆä¹Ÿå°±æ˜¯ä½ çš„ç‰¹å¾å¯ä»¥åšåµŒå…¥æ“ä½œï¼Œç„¶åç»™ç½‘ç»œæ¥æ›´å¥½çš„å¤„ç†ï¼‰
        D = x_emb.shape[-2]
        # [B, M, D, N] -> [B, M*D, N]
        x = rearrange(x_emb, 'b m d n -> b (m d) n')
        
        # TODO ç»“æ„é‡å‚
        # [B, M*D, N] -> [B, M*D, N]
        x_l = self.bn(self.dw_conv(x))
        x_s = self.bn_samll(self.dw_conv_small(x))
        x = x_l + x_s

        # [B, M*D, N] -> [B, M*D, N]
        x = self.conv_ffn1(x)

        # [B, M*D, N] -> [B, M, D, N]
        x = rearrange(x, 'b (m d) n -> b m d n', d=D)
        # [B, M, D, N] -> [B, D, M, N]
        x = x.permute(0, 2, 1, 3)
        # [B, D, M, N] -> [B, D*M, N]
        x = rearrange(x, 'b d m n -> b (d m) n')

        # [B, D*M, N] -> [B, D*M, N]
        x = self.conv_ffn2(x)

        # [B, D*M, N] -> [B, D, M, N]
        x = rearrange(x, 'b (d m) n -> b d m n', d=D)
        # [B, D, M, N] -> [B, M, D, N]
        x = x.permute(0, 2, 1, 3)

        out = x + x_emb

        return out  # out: [B, M, D, N] å’Œ TCN ä¸€æ ·è¾“å…¥è¾“å‡ºç›¸åŒ


'''
æ–‡ä»¶è¯´æ˜ï¼š
1. TCN ä¸º TCN çš„é¢„æµ‹ä»»åŠ¡ï¼Œè¾“å…¥ä¸º x: [B, M, L] ï¼ˆå¯ä»¥ä½¿ç”¨ embedding or ä¸ä½¿ç”¨ç›´æ¥çœ‹æƒ…å†µï¼‰

2. ModernTCN_DC è„‰å†²æµæ ‡æ³¨ä»»åŠ¡ï¼š
    è¾“å…¥ä¸º (64,5,1024) 5 ä¸ªæ—¶åºç‰¹å¾ï¼Œé•¿åº¦ 1024
    è¾“å‡ºä¸º (64,1024,12),1024 é•¿åº¦åºåˆ—å¯¹åº” 12 ä¸ªç±»åˆ«

3. ModernTCN_mnist ä¸º TCN çš„åˆ†ç±»ä»»åŠ¡)

å‚æ•°è¯´æ˜ï¼š
# Bï¼šbatch size ğŸ˜€
# Mï¼šå¤šå˜é‡åºåˆ—çš„å˜é‡æ•° ğŸ˜€ æˆ‘çš„ä¸º 5  channel
# Lï¼šè¿‡å»åºåˆ—çš„é•¿åº¦ ğŸ˜€ æˆ‘çš„ä¸º 1024  input
# T: é¢„æµ‹åºåˆ—çš„é•¿åº¦ ğŸ˜€ æˆ‘è¦æ”¹æˆåˆ†ç±»ä»»åŠ¡ è¿™é‡Œç”¨ä¸åˆ° output

# embedding å‚æ•°ï¼š
# N: åˆ† Patch å Patch çš„ä¸ªæ•°
# Dï¼šæ¯ä¸ªå˜é‡çš„é€šé“æ•°
# Pï¼škernel size of embedding layer
# Sï¼šstride of embedding layer
'''

# TCN æœ€æ–°å·¥ä½œ ModernTCN ,å…¶å®ï¼Œå¯ä»¥å†™ä¿®æ”¹çš„ç‰ˆæœ¬ï¼Œè¿˜æ–¹ä¾¿æµ‹è¯•


class ModernTCN_DC(nn.Module):  # T åœ¨é¢„æµ‹ä»»åŠ¡å½“ä¸­ä¸ºé¢„æµ‹çš„é•¿åº¦ï¼Œå¯ä»¥æ›´æ¢ä¸ºè¾“å‡ºçš„ç§ç±» num_classes
    def __init__(self, M, L, num_classes, D=64, P=1, S=1, kernel_size=51, r=2, num_layers=24, pos_D=128):  # å¦‚æœèƒ½æ”¶æ•›å°±ä¸€ç‚¹ä¸€ç‚¹å¢åŠ ï¼Œåœ¨åŸæ¥è·‘é€šçš„é‡Œé¢å±‚æ•°ä¸º
        # M, L, num_classes,
        super(ModernTCN_DC, self).__init__()
        # æ·±åº¦åˆ†ç¦»å·ç§¯è´Ÿè´£æ•è·æ—¶åŸŸå…³ç³»
        self.num_layers = num_layers
        # N = L // S
        # ç»éªŒè¯ï¼Œå¢åŠ  emb çš„è¯ä¼šå¢åŠ æ¨¡å‹éœ‡è¡
        # self.embed_layer = EmbeddingPos(pos_D, P, S, D)
        # self.embed_layer = Embedding(P, S, D)

        self.backbone = nn.ModuleList(
            [ModernTCNBlock(M, D, kernel_size, r) for _ in range(num_layers)])

        # [B, M, D, N] -> [B, M, D, N]
        # rearrange å®ç° [B, M, D, N] -> [B, M, D*N] ã€64,5,D*Nã€‘ (D = 64)
        # D*N# N: åˆ† Patch å Patch çš„ä¸ªæ•°# Dï¼šæ¯ä¸ªå˜é‡çš„é€šé“æ•° D*N ğŸ˜€ è¾“å‡ºä¸ºä½ çš„è¾“å‡º 1024 åŸåºåˆ—é•¿åº¦ï¼Œç„¶åæ¥åˆ†ç±»çš„ä»»åŠ¡å’Œ loss å»åšï¼Œå…·ä½“å¯ä»¥å‚è€ƒ TCN ä¸­åˆ†ç±»ä»»åŠ¡å½“ä¸­ä»»åŠ¡å¤´æ€ä¹ˆå†™çš„ï¼ˆæ³¨ï¼Œä¹‹å‰çš„å·¥ä½œå¯èƒ½æ²¡æœ‰ç‰¹å¾ embedding çš„æ˜ å°„æµç¨‹ï¼Œä½†ä»»åŠ¡è®¾è®¡å’Œä¿®æ”¹æ˜¯å¥½çš„ï¼‰

        # w/o pool
        self.classificationhead = nn.Linear(D * M, num_classes)

        # # with pool
        # self.classificationhead = nn.Linear(D, num_classes)

    def forward(self, pos):
        # L = N = 1024 åºåˆ—é•¿ (P=1, S=1 æ—¶)
        # B = batch size
        # pos: [B, L=1024, M=5, pos_D=128]

        # [B, L=1024, M=5, pos_D=128] -> [B, M=5, D=128, L=1024]
        x_emb = rearrange(pos, 'b l m d -> b m d l')
        # x_emb = self.embed_layer(x_emb) # [B, L=1024, M=5, pos_D=128] -> [B, M=5, D=128, L=1024]

        for i in range(self.num_layers):  # è¿‡å‡ å±‚ block
            x_emb = self.backbone[i](x_emb)  # [B, M, D, N] -> [B, M, D, N]

        # åœ¨å±•å¹³ä¹‹å‰ï¼Œ[64, 5, 64, 1024] è¦åšåºåˆ—æ ‡æ³¨ä»»åŠ¡ åˆ™ [64,5,1024,12] å°† 5 ä¸ªç‰¹å¾ç»´åº¦èšåˆå¾—åˆ° [64,1024,12]
        # æœ¬è´¨æ˜¯ [B, M, D, N] -> [B, L, classes],å…¶ä¸­ L ä¸º 1024ï¼Œclasses ä¸º 12ï¼Œä¸” N = L // S
        # å¯ä»¥è€ƒè™‘ä½¿ç”¨æ›´å¤æ‚çš„æ± åŒ–æ–¹å¼ã€æ·»åŠ  dropout ç­‰æ¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

        # Flatten å°†é¢„æµ‹çš„é•¿åº¦æ‹‰å¼€ï¼ŒæŠŠåµŒå…¥çš„ç»´åº¦æ‹‰å¼€
        # [B, M, D, N] -> [B, M*D, N]
        cls1 = rearrange(x_emb, 'b m d n -> b (m d) n')

        # maxpool
        # cls1 = torch.max(x_emb, dim=1)[0]    # [B, M, D, N] -> [B, D, N]

        # è½¬æ¢ä¸º [64, 1024, 64]
        cls1 = cls1.permute(0, 2, 1)  # [64, 64, 1024] -> [64, 1024, 64]

        # è¾“å‡ºä¸º [64, 1024,12]
        # [64, 1024, 1, 64] -> [64, 1024, 1, 12]
        out1 = self.classificationhead(cls1)

        return out1


# æµ‹è¯•å•å…ƒ
if __name__ == '__main__':
    # (64,5,1024) æµ‹è¯•

    past_series = torch.rand(20, 1024, 5, 128).cuda()

    # model = ModernTCN_mnist(5, 1024, 10)  # å¯¹åº”çš„å‚æ•°å«ä¹‰ä¸º M, L, T, 4 ä¸ªåºåˆ—ç‰¹å¾ï¼Œ96 åŸè¾“å…¥é•¿åº¦ 96ï¼Œé¢„æµ‹è¾“å‡ºé•¿åº¦ä¸º 192

    # å¯¹åº”çš„å‚æ•°å«ä¹‰ä¸º M, L, T, 4 ä¸ªåºåˆ—ç‰¹å¾ï¼Œ96 åŸè¾“å…¥é•¿åº¦ 96ï¼Œé¢„æµ‹è¾“å‡ºé•¿åº¦ä¸º 192
    model = ModernTCN_DC(5, 1024, 12, 128).cuda()

    pred_series = model(past_series)
    print(pred_series.shape)
